#!/usr/bin/env python3

#!/usr/bin/env pythonimport cv2
import os
import queue
import cv2
import time
import rospy
import torch
import operator
import ros_numpy
import numpy as np
import message_filters

from sensor_msgs import point_cloud2
from std_msgs.msg import String, Header
import jsk_recognition_msgs.msg as jsk_msgs
import darknet_ros_msgs.msg as darknet_msgs
from geometry_msgs.msg import PoseArray, Pose
from sensor_msgs.msg import Image, PointCloud2, PointField

import frustum_pointnet.kitti.kitti_util as utils
from frustum_pointnet.models.frustum_pointnets import FrustumPointNetv1
from frustum_pointnet.models.model_util import g_type2class, g_class2type, g_type2onehotclass
from frustum_pointnet.train.provider import from_prediction_to_label_format, rotate_pc_along_y

MODEL_ROOT = "/hdd/catkin_ws/src/frustum_pointnet/src/frustum_pointnet/log/run3_carpedcyc_kitti_2022-05-05-22/acc0.641093-epoch143.pth"
DATASET_ROOT = "/hdd/Thesis/SiameseTracker/dataset/KITTI/training"
IMG_TAG = "image_02"
SEQ = "0000"
PC_TAG = "velodyne"
IMG_SEQUENCE = os.path.join(IMG_TAG, SEQ)
PC_SEQUENCE = os.path.join(PC_TAG, SEQ)
SEQ_CALIB_PATH = os.path.join(DATASET_ROOT, "calib", "0000.txt")
IMAGE_DATASET_PATH = os.path.join(DATASET_ROOT, IMG_SEQUENCE)
PC_DATASET_PATH = os.path.join(DATASET_ROOT, PC_SEQUENCE)
LABELS_PATH = os.path.join(DATASET_ROOT, "label_02", SEQ + ".txt")

darknet_to_pointnet = {'person': 'Pedestrian', 'car': 'Car'}

class KittiSubscriber:
    def __init__(self) -> None:
        self.load_pretrained = MODEL_ROOT
        self.n_classes = 3
        self.calib = utils.Calibration(SEQ_CALIB_PATH)

        print('Initializing model...')
        self.model = FrustumPointNetv1(n_classes=self.n_classes)

        if torch.cuda.is_available():
            self.model = self.model.cuda()

        # load pre-trained model
        if self.load_pretrained:
            print('Loading model from ', self.load_pretrained)
            ckpt = torch.load(self.load_pretrained)
            self.model.load_state_dict(ckpt['model_state_dict'])

        self.model.eval()

        # ROS
        rospy.init_node('frustrum_pointnet_ros', anonymous=True)


        # Publisher of the prediction result
        self.prediction_3d = rospy.Publisher(
            'prediction_3d_publisher', jsk_msgs.BoundingBoxArray, queue_size=1)
        # Publisher of the prediction result
        self.frustum_pc = rospy.Publisher(
            'frustum_pc_publisher', PointCloud2, queue_size=1)

        # Subscriber to the data stream
        ts = message_filters.ApproximateTimeSynchronizer([
             message_filters.Subscriber('/image_publisher', Image, queue_size=100),
             message_filters.Subscriber('/depth_publisher', PointCloud2, queue_size=100),
             message_filters.Subscriber(
                 '/gt_bb_publisher_3D', jsk_msgs.BoundingBoxArray, queue_size=100),
             message_filters.Subscriber(
                 '/bounding_box_2d_publisher', darknet_msgs.BoundingBoxes, queue_size=100),
        ], queue_size=100, slop=0.2, allow_headerless=True)

        ts.registerCallback(self.detect_3d_object)

        rospy.spin()

    def array2pc(self, points):
        '''Convert numpy array in ros PointCloud2 msg'''
        fields = [PointField('x', 0, PointField.FLOAT32, 1),
                  PointField('y', 4, PointField.FLOAT32, 1),
                  PointField('z', 8, PointField.FLOAT32, 1)]

        header = Header()
        header.frame_id = "base_link"
        header.stamp = rospy.Time.now()

        print(points.shape)
        points = points.squeeze().reshape(-1, 3)
        # print(points)

        return point_cloud2.create_cloud(header, fields, points)

    def pub_frustum(self, frustums, timestamp):
        '''Publish lidar points in frustums'''
        if len(frustums) == 0:
            return

        frustum = np.vstack(frustums)

        lidar_window_msg = self.array2pc(frustum)
        lidar_window_msg.header.stamp = timestamp
        self.frustum_pc.publish(lidar_window_msg)


    def pub_pred_bbox(self, predictions, timestamp):
        '''Publish predicted object 3D bounding boxes'''

        bbox_array_3D = jsk_msgs.BoundingBoxArray()
        bbox_array_3D.boxes = []
        bbox_array_3D.header = Header()
        bbox_array_3D.header.stamp = timestamp
        bbox_array_3D.header.frame_id = "base_link"

        i = 0
        while i < len(predictions):
            tx, ty, tz, h, w, l, ry = predictions[i]

            # 3D Bounding box
            bbox_3D = jsk_msgs.BoundingBox()
            bbox_3D.header.frame_id = "base_link"
            bbox_3D.dimensions.x = w
            bbox_3D.dimensions.y = l
            bbox_3D.dimensions.z = h
            bbox_3D.pose.position.x = tz
            bbox_3D.pose.position.y = -tx
            bbox_3D.pose.position.z = -ty/2

            roll, pitch, yaw = 0, 0, ry
            qx = np.sin(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) - \
                np.cos(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)
            qy = np.cos(roll/2) * np.sin(pitch/2) * np.cos(yaw/2) + \
                np.sin(roll/2) * np.cos(pitch/2) * np.sin(yaw/2)
            qz = np.cos(roll/2) * np.cos(pitch/2) * np.sin(yaw/2) - \
                np.sin(roll/2) * np.sin(pitch/2) * np.cos(yaw/2)
            qw = np.cos(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) + \
                np.sin(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)

            bbox_3D.pose.orientation.x = qx
            bbox_3D.pose.orientation.y = qy
            bbox_3D.pose.orientation.z = qz
            bbox_3D.pose.orientation.w = qw

            bbox_array_3D.boxes.append(bbox_3D)

            i += 1

        self.prediction_3d.publish(bbox_array_3D)

    def detect_3d_object(self, image, depth, gt_3d_list, gt_2d_list):
        ''' Callback to detect 3D objects from 2D bounding box'''

        timestamp = image.header.stamp
        cv_image = ros_numpy.numpify(image)

        bbox_2D_list = gt_2d_list.bounding_boxes
        predictions_list = []

        frustums = []

        bbox_3D_list = gt_3d_list.boxes
        start = time.time()

        for bbox_2D, bbox_3D in zip(bbox_2D_list, bbox_3D_list):
            # Get the 3D points inside the 2D bounding box
            xmin, ymin, xmax, ymax = bbox_2D.xmin, bbox_2D.ymin, bbox_2D.xmax, bbox_2D.ymax
            w, h = (xmax - xmin)/2, (ymax - ymin)/2
            cx, cy = xmin + w // 2, ymin + h//2

            # cv2.rectangle(cv_image, pt1=(xmin, ymin), pt2=(xmax, ymax), color=(
            #     0, 255, 0), thickness=3, lineType=cv2.LINE_4)
            # cv2.imshow("test", cv_image)
            # cv2.waitKey(0)

            pc = ros_numpy.numpify(depth)

            pc_3d_coord = np.zeros((pc.shape[0], 4), dtype=np.float32)
            pc_3d_coord[:, 0] = pc['x']
            pc_3d_coord[:, 1] = pc['y']
            pc_3d_coord[:, 2] = pc['z']
            pc_3d_coord[:, 3] = pc['intensity']

            pc_image_coord = self.calib.project_velo_to_image(
                pc_3d_coord[:, :3])

            box_fov_inds = (pc_image_coord[:, 0] < xmax) & \
                           (pc_image_coord[:, 0] > xmin) & \
                           (pc_image_coord[:, 1] < ymax) & \
                           (pc_image_coord[:, 1] > ymin)

            pc_in_box_fov = pc_3d_coord[box_fov_inds, :]

            # Make sure class exists and pointcloud is not empty
            if bbox_2D.Class not in darknet_to_pointnet.keys() or pc_in_box_fov.shape[0] <= 1:
                continue
            
            # Correct classes to right format
            cls_type = darknet_to_pointnet[bbox_2D.Class]

            one_hot_vec = np.zeros((3))
            one_hot_vec[g_type2onehotclass[cls_type]] = 1

            input_data = {}

            # Get frustrum angle
            box2d_center = np.array([cx, cy])
            uvdepth = np.zeros((1, 3))
            uvdepth[0, 0:2] = box2d_center
            uvdepth[0, 2] = 20  # some random depth
            box2d_center_rect = self.calib.project_image_to_rect(uvdepth)
            frustum_angle = -1 * \
                np.arctan2(box2d_center_rect[0, 2], box2d_center_rect[0, 0])
            frustum_angle += np.pi / 2.0

            # Transform to frustum coordinate frame 
            # F-PointNet only accepts input in camera reference frame
            pc_in_box_fov = self.calib.project_velo_to_ref(
                pc_in_box_fov[:, 0:3])
            pc_in_box_fov_rotated = rotate_pc_along_y(
                np.copy(pc_in_box_fov), frustum_angle)

            # Send to device
            pc_in_box_fov_rotated = torch.tensor(
                pc_in_box_fov_rotated, dtype=torch.float32).unsqueeze(0).cuda()  # 1x4xn
            one_hot_vec = torch.tensor(
                one_hot_vec, dtype=torch.float32).cuda()  # 1x3

            # Predict
            input_data['point_cloud'] = pc_in_box_fov_rotated.transpose(2, 1)
            input_data['one_hot'] = one_hot_vec
            net_out = self.model(input_data)
            center = net_out['box3d_center'].cpu().detach().numpy()[0]

            angle_class = np.argmax(
                net_out['heading_scores'].cpu().detach().numpy())
            angle_res = net_out['heading_residual'].cpu().detach().numpy()[
                0][angle_class]

            size_class = np.argmax(
                net_out['size_scores'].cpu().detach().numpy())
            size_res = net_out['size_residual'].cpu().detach().numpy()[
                0][size_class]
            rot = frustum_angle

            h, w, l, tx, ty, tz, ry = from_prediction_to_label_format(
                center, angle_class, angle_res,
                size_class, size_res, rot)

            object_pts = net_out['object_points'].cpu().detach().numpy()

            predictions_list.append([tx, ty, tz, h, w, l, -ry])

            pc_in_box_fov_rotated = pc_in_box_fov_rotated.cpu().detach().numpy().squeeze()
            frustums.append(pc_in_box_fov_rotated)


        finish = time.time()
        print("Framerate:", 1/(finish - start))
        self.pub_frustum(frustums, timestamp)
        self.pub_pred_bbox(predictions_list, timestamp)


def print_projection_cv2(points, image):
    """ project converted velodyne points into camera image """
    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    for i in range(points.shape[0]):
        cv2.circle(hsv_image, (np.int32(points[i][0]), np.int32(
            points[i][1])), 2, (0, 255, 255), -1)

    image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)

    cv2.imshow('', np.copy(image))
    cv2.waitKey(0)


if __name__ == '__main__':
    print("Starting frustrum pointnet subscriber node...")

    sub = KittiSubscriber()
