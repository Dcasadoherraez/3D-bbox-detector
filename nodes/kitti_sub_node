#!/usr/bin/env python3

#!/usr/bin/env pythonimport cv2
import os
import queue
import cv2
import time
import rospy
import torch
import operator
import ros_numpy
import numpy as np
import message_filters

from sensor_msgs import point_cloud2
from std_msgs.msg import String, Header
import jsk_recognition_msgs.msg as jsk_msgs
import darknet_ros_msgs.msg as darknet_msgs
from geometry_msgs.msg import PoseArray, Pose
from sensor_msgs.msg import Image, PointCloud2, PointField

from frustum_pointnet.ros.sub_node_class import FPointnetNode
import frustum_pointnet.kitti.kitti_util as utils
from frustum_pointnet.models.frustum_pointnets import FrustumPointNetv1
from frustum_pointnet.models.model_util import g_type2class, g_class2type, g_type2onehotclass
from frustum_pointnet.train.provider import from_prediction_to_label_format, rotate_pc_along_y

MODEL_ROOT = "/hdd/catkin_ws/src/frustum_pointnet/src/frustum_pointnet/log/run3_carpedcyc_kitti_2022-05-05-22/acc0.641093-epoch143.pth"
DATASET_ROOT = "/hdd/Thesis/SiameseTracker/dataset/KITTI/training"
IMG_TAG = "image_02"
SEQ = "0000"
PC_TAG = "velodyne"
IMG_SEQUENCE = os.path.join(IMG_TAG, SEQ)
PC_SEQUENCE = os.path.join(PC_TAG, SEQ)
SEQ_CALIB_PATH = os.path.join(DATASET_ROOT, "calib", "0000.txt")
IMAGE_DATASET_PATH = os.path.join(DATASET_ROOT, IMG_SEQUENCE)
PC_DATASET_PATH = os.path.join(DATASET_ROOT, PC_SEQUENCE)
LABELS_PATH = os.path.join(DATASET_ROOT, "label_02", SEQ + ".txt")

darknet_to_pointnet = {'person': 'Pedestrian', 'car': 'Car'}

class KittiSubscriber(FPointnetNode):
    def __init__(self, model_root, n_classes) -> None:
        super().__init__(model_root, n_classes)

        self.calib = utils.Calibration(SEQ_CALIB_PATH)

        # ROS
        rospy.init_node('frustrum_pointnet_ros', anonymous=True)

        image_topic_sub =   rospy.get_param('fpointnet_image_topic_sub')
        depth_topic_sub =   rospy.get_param('fpointnet_depth_topic_sub')
        bbox_2d_topic_sub = rospy.get_param('fpointnet_bbox_2d_topic_sub')
        bbox_3d_topic_pub = rospy.get_param('fpointnet_bbox_3d_topic_pub')
        frustum_pc_pub =    rospy.get_param('fpointnet_frustum_topic_pub')

        # Publisher of the prediction result
        self.prediction_3d_pub = rospy.Publisher(bbox_3d_topic_pub, jsk_msgs.BoundingBoxArray, queue_size=1)
        self.frustum_pc_pub = rospy.Publisher(frustum_pc_pub, PointCloud2, queue_size=1)

        # Subscriber to the data stream
        ts = message_filters.ApproximateTimeSynchronizer([
             message_filters.Subscriber(image_topic_sub, Image, queue_size=100),
             message_filters.Subscriber(depth_topic_sub, PointCloud2, queue_size=100),
             message_filters.Subscriber(bbox_2d_topic_sub, darknet_msgs.BoundingBoxes, queue_size=100),
        ], queue_size=100, slop=0.2, allow_headerless=True)

        ts.registerCallback(self.detect_3d_object)

        rospy.spin()

    def detect_3d_object(self, image, depth, gt_2d_list):
        ''' Callback to detect 3D objects from 2D bounding box'''

        timestamp = image.header.stamp
        cv_image = ros_numpy.numpify(image)

        bbox_2D_list = gt_2d_list.bounding_boxes
        predictions_list = []

        frustums = []

        start = time.time()

        for bbox_2D in bbox_2D_list:
            # Get the 3D points inside the 2D bounding box
            xmin, ymin, xmax, ymax = bbox_2D.xmin, bbox_2D.ymin, bbox_2D.xmax, bbox_2D.ymax
            w, h = (xmax - xmin)/2, (ymax - ymin)/2
            cx, cy = xmin + w // 2, ymin + h//2

            # cv2.rectangle(cv_image, pt1=(xmin, ymin), pt2=(xmax, ymax), color=(
            #     0, 255, 0), thickness=3, lineType=cv2.LINE_4)
            # cv2.imshow("test", cv_image)
            # cv2.waitKey(0)

            pc = ros_numpy.numpify(depth)

            pc_3d_coord = np.zeros((pc.shape[0], 4), dtype=np.float32)
            pc_3d_coord[:, 0] = pc['x']
            pc_3d_coord[:, 1] = pc['y']
            pc_3d_coord[:, 2] = pc['z']
            pc_3d_coord[:, 3] = pc['intensity']

            pc_image_coord = self.calib.project_velo_to_image(
                pc_3d_coord[:, :3])

            box_fov_inds = (pc_image_coord[:, 0] < xmax) & \
                           (pc_image_coord[:, 0] > xmin) & \
                           (pc_image_coord[:, 1] < ymax) & \
                           (pc_image_coord[:, 1] > ymin)

            pc_in_box_fov = pc_3d_coord[box_fov_inds, :]

            # Make sure class exists and pointcloud is not empty
            if bbox_2D.Class not in darknet_to_pointnet.keys() or pc_in_box_fov.shape[0] <= 1:
                continue
            
            # Correct classes to right format
            cls_type = darknet_to_pointnet[bbox_2D.Class]

            one_hot_vec = np.zeros((3))
            one_hot_vec[g_type2onehotclass[cls_type]] = 1

            input_data = {}

            # Get frustrum angle
            box2d_center = np.array([cx, cy])
            uvdepth = np.zeros((1, 3))
            uvdepth[0, 0:2] = box2d_center
            uvdepth[0, 2] = 20  # some random depth
            box2d_center_rect = self.calib.project_image_to_rect(uvdepth)
            frustum_angle = -1 * \
                np.arctan2(box2d_center_rect[0, 2], box2d_center_rect[0, 0])
            frustum_angle += np.pi / 2.0

            # Transform to frustum coordinate frame 
            # F-PointNet only accepts input in camera reference frame
            pc_in_box_fov = self.calib.project_velo_to_ref(
                pc_in_box_fov[:, 0:3])
            pc_in_box_fov_rotated = rotate_pc_along_y(
                np.copy(pc_in_box_fov), frustum_angle)

            # Send to device
            pc_in_box_fov_rotated = torch.tensor(
                pc_in_box_fov_rotated, dtype=torch.float32).unsqueeze(0).cuda()  # 1x4xn
            one_hot_vec = torch.tensor(
                one_hot_vec, dtype=torch.float32).cuda()  # 1x3

            # Predict
            input_data['point_cloud'] = pc_in_box_fov_rotated.transpose(2, 1)
            input_data['one_hot'] = one_hot_vec
            net_out = self.model(input_data)
            center = net_out['box3d_center'].numpy()[0]

            angle_class = np.argmax(
                net_out['heading_scores'].numpy())
            angle_res = net_out['heading_residual'].numpy()[
                0][angle_class]

            size_class = np.argmax(
                net_out['size_scores'].numpy())
            size_res = net_out['size_residual'].numpy()[
                0][size_class]
            rot = frustum_angle

            h, w, l, tx, ty, tz, ry = from_prediction_to_label_format(
                center, angle_class, angle_res,
                size_class, size_res, rot)

            object_pts = net_out['object_points'].numpy()

            predictions_list.append([tx, ty, tz, h, w, l, -ry])

            pc_in_box_fov_rotated = pc_in_box_fov_rotated.cpu().detach().numpy().squeeze()
            frustums.append(pc_in_box_fov_rotated)


        finish = time.time()
        print("Framerate:", 1/(finish - start))

        if len(frustums) > 0:
            frustum_msgs = self.get_frustum_msg(frustums, timestamp)
            self.frustum_pc_pub.publish(frustum_msgs)
        
        prediction_msgs = self.get_pred_bbox_msg(predictions_list, timestamp)
        self.prediction_3d_pub.publish(prediction_msgs)


def print_projection_cv2(points, image):
    """ project converted velodyne points into camera image """
    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    for i in range(points.shape[0]):
        cv2.circle(hsv_image, (np.int32(points[i][0]), np.int32(
            points[i][1])), 2, (0, 255, 255), -1)

    image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)

    cv2.imshow('', np.copy(image))
    cv2.waitKey(0)


if __name__ == '__main__':
    print("Starting frustrum pointnet subscriber node for KITTI...")

    sub = KittiSubscriber(MODEL_ROOT, n_classes=3)
